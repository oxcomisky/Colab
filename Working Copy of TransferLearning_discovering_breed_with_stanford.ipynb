{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Working Copy of TransferLearning_discovering_breed_with_stanford.ipynb","provenance":[{"file_id":"1NGCguPKRxnQeDu_8bdf01tV9fNiDSKZD","timestamp":1636131783953},{"file_id":"1c8SsnMNuEvKp5ZkBGTxVgTzUVEsZ7rwP","timestamp":1635624817447},{"file_id":"1suwbmnDLCAQuP9PrUozYHHjx0k_pMv8v","timestamp":1635346048622}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"JMDFRYQmHqEV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636118602585,"user_tz":300,"elapsed":18243,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"4f0dab8a-fb16-42e9-d6b3-4bb48bee1c61"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"dH3bJx7sGzvt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636118620584,"user_tz":300,"elapsed":18001,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"91658369-7993-4d27-ae10-08c61fa4b7db"},"source":["!dir\n","!python -m pip install \"dask[complete]\"\n","!python -m pip install pynndescent\n","!python -m pip install tqdm\n","!python -m pip install umap"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n","Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n","Collecting fsspec>=0.6.0\n","  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: bokeh>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.3.3)\n","Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.3.0)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.19.5)\n","Collecting partd>=0.3.10\n","  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.1.5)\n","Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (0.11.1)\n","Requirement already satisfied: PyYaml in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (3.13)\n","Collecting distributed>=2.0\n","  Downloading distributed-2021.10.0-py3-none-any.whl (791 kB)\n","\u001b[K     |████████████████████████████████| 791 kB 43.6 MB/s \n","\u001b[?25hRequirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (5.1.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.8.2)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (3.7.4.3)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (21.0)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.11.3)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.4.0)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (7.1.2)\n","  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n","\u001b[K     |████████████████████████████████| 786 kB 69.4 MB/s \n","\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.0.0)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.0.2)\n","Collecting cloudpickle>=0.2.1\n","  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (5.4.8)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.7.0)\n","Collecting distributed>=2.0\n","  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n","\u001b[K     |████████████████████████████████| 779 kB 59.8 MB/s \n","\u001b[?25h  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n","\u001b[K     |████████████████████████████████| 778 kB 57.1 MB/s \n","\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n","\u001b[K     |████████████████████████████████| 776 kB 73.2 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n","\u001b[K     |████████████████████████████████| 769 kB 71.0 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n","\u001b[K     |████████████████████████████████| 766 kB 29.2 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 64.6 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n","\u001b[K     |████████████████████████████████| 722 kB 61.2 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n","\u001b[K     |████████████████████████████████| 722 kB 75.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n","\u001b[K     |████████████████████████████████| 715 kB 78.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n","\u001b[K     |████████████████████████████████| 705 kB 72.0 MB/s \n","\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n","\u001b[K     |████████████████████████████████| 699 kB 81.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n","\u001b[K     |████████████████████████████████| 696 kB 78.7 MB/s \n","\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n","\u001b[K     |████████████████████████████████| 684 kB 76.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n","\u001b[K     |████████████████████████████████| 679 kB 58.5 MB/s \n","\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n","\u001b[K     |████████████████████████████████| 675 kB 72.9 MB/s \n","\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n","\u001b[K     |████████████████████████████████| 675 kB 72.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n","\u001b[K     |████████████████████████████████| 672 kB 76.6 MB/s \n","\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n","\u001b[K     |████████████████████████████████| 671 kB 77.9 MB/s \n","\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n","\u001b[K     |████████████████████████████████| 669 kB 77.8 MB/s \n","\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n","\u001b[K     |████████████████████████████████| 656 kB 79.7 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (57.4.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=1.0.0->dask[complete]) (2.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh>=1.0.0->dask[complete]) (2.4.7)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[complete]) (2018.9)\n","Collecting locket\n","  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh>=1.0.0->dask[complete]) (1.15.0)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0->dask[complete]) (1.0.1)\n","Installing collected packages: locket, cloudpickle, partd, fsspec, distributed\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: distributed\n","    Found existing installation: distributed 1.25.3\n","    Uninstalling distributed-1.25.3:\n","      Successfully uninstalled distributed-1.25.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\u001b[0m\n","Successfully installed cloudpickle-2.0.0 distributed-2.30.1 fsspec-2021.10.1 locket-0.2.1 partd-1.2.0\n","Collecting pynndescent\n","  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.22.2.post1)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.4.1)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.51.2)\n","Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.34.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.2->pynndescent) (57.4.0)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.2->pynndescent) (1.19.5)\n","Building wheels for collected packages: pynndescent\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=4891b19913d340d5a07a14cc988e4057eba65bcf59c82f0d55c9826e6c4ca688\n","  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n","Successfully built pynndescent\n","Installing collected packages: pynndescent\n","Successfully installed pynndescent-0.5.5\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Collecting umap\n","  Downloading umap-0.1.1.tar.gz (3.2 kB)\n","Building wheels for collected packages: umap\n","  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3564 sha256=e36a0eaad1bc4289c0f6f351c6ed3efcfc308a0ac13e378ee12f765e385b90ce\n","  Stored in directory: /root/.cache/pip/wheels/65/55/85/945cfb3d67373767e4dc3e9629300a926edde52633df4f0efe\n","Successfully built umap\n","Installing collected packages: umap\n","Successfully installed umap-0.1.1\n"]}]},{"cell_type":"code","metadata":{"id":"7PhefgVyEoZp"},"source":["# basic imports \n","import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import dask.dataframe as dd\n","from tqdm import tqdm\n","import keras\n","\n","# plotting\n","import matplotlib.pyplot as plt\n","\n","# improving plots\n","from IPython.display import set_matplotlib_formats\n","set_matplotlib_formats('retina')\n","plt.style.use('bmh')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6naBwrQEoZr"},"source":["## Custom utilities\n","\n","I build some custom utilities to isolate some core functionalities of my code. I'll explain them in the post, but if you want to dig deeper please refer to [this script]() on the repository."]},{"cell_type":"code","metadata":{"id":"TvV306q8EoZr"},"source":["# basic imports \n","import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import dask.dataframe as dd\n","from tqdm import tqdm\n","\n","# plotting\n","import matplotlib.pyplot as plt\n","\n","# improving plots\n","from IPython.display import set_matplotlib_formats\n","set_matplotlib_formats('retina')\n","plt.style.use('bmh')\n","\n","# using a pre-trained net\n","#from tensorflow.keras.applications.xception import preprocess_input\n","#from tensorflow.keras.applications.vgg19 import preprocess_input\n","from tensorflow.keras.applications.densenet import preprocess_input\n","#from tensorflow.keras.applications.inception_v3 import preprocess_input\n","\n","from tensorflow.keras.preprocessing import image\n","\n","# nearest neighbors\n","from sklearn.neighbors import KDTree\n","from pynndescent import NNDescent\n","\n","# tools for creating embedding plots\n","from sklearn.cluster import KMeans\n","from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","\n","\n","# function to load data\n","def build_metadata():\n","\n","    # list for meta_df\n","    meta_df = []\n","\n","    # traversing folders and images\n","    for base_path, breed_folder, imgs in tqdm(os.walk('/content/drive/MyDrive/CS465/project/Images')):\n","        for img in imgs:\n","\n","            # gathering metadata\n","            pet_id = f'{base_path}/{img}'\n","            breed = '-'.join(base_path.split('/')[-1].split('-')[1:])\n","\n","            # dataframe with this info\n","            temp_df = pd.DataFrame({'breed': breed}, index=pd.Index([pet_id], name='pet_id'))\n","            meta_df.append(temp_df)\n","\n","    # returning full dataframe\n","    meta_df = pd.concat(meta_df)\n","    return meta_df\n","\n","def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i:i + n]\n","\n","# function to read all images into array\n","def read_images(pet_ids, target_width=244, target_height=244):\n","    \n","    # list with images and ids\n","    images = []\n","    processed_ids = []\n","    \n","    # loop for each pet id in the main dataframe\n","    for pet_id in tqdm(pet_ids):\n","        \n","        try:\n","            \n","            # reading image and putting it into machine format\n","            img = image.load_img(pet_id, target_size=(target_width, target_height))\n","            img = image.img_to_array(img)\n","            img = preprocess_input(img)\n","            \n","            # saving\n","            images.append(img)\n","            processed_ids.append(pet_id)\n","        \n","        # do nothing if passes\n","        except:\n","            pass\n","        \n","    return np.array(images), np.array(processed_ids)\n","\n","# function to extract and save features from images\n","def extract_features(pet_ids, extractor):\n","    \n","    # getting features iterating\n","    features_df = pd.DataFrame()\n","    for pet_chunk in chunks(pet_ids, 2048):\n","  \n","        # reading and processing images\n","        images, processed_ids = read_images(pet_chunk)\n","        result = extractor.predict(images, batch_size=128, verbose=1, use_multiprocessing=True)\n","        result = pd.DataFrame(result, index = pd.Index(processed_ids, name='pet_id'))\n","        features_df = pd.concat([features_df, result])\n","\n","    # saving df\n","    return features_df\n","\n","# report of zeca's comparables\n","def get_prototypes_report(path, index, extractor, transform_fn, meta_df, features_df, k=50):\n","    \n","    # features from zeca\n","    features = extract_features([path], extractor)\n","\n","    # querying zeca NNs\n","    nns = index.query(transform_fn(features), k=k)\n","    \n","    # breeds of comparable dogs\n","    comps_breed = meta_df.iloc[nns[0][0]]['breed']\n","    breed_counts = comps_breed.value_counts()\n","    print('Most Frequent Breeds:')\n","    print((breed_counts/breed_counts.sum()).head(10))\n","    \n","    # comps\n","    comps_fig_path = features_df.index[nns[0][0]].values\n","\n","    # opening matplotlib figure\n","    fig = plt.figure(figsize=(20, 10), dpi=100)\n","\n","    # loop for all figures\n","    for i, path in enumerate(comps_fig_path):\n","        plt.subplot(5, 10, i+1)\n","        plt.imshow(plt.imread(path))\n","        plt.title(comps_breed.iloc[i], fontsize=9)\n","        plt.grid(b=None)\n","        plt.xticks([]); plt.yticks([])\n","    \n","def plot_dog_atlas(embed, meta_df, title, ax):\n","\n","    # fitting kmeans to get evenly spaced points on MAP\n","    km = KMeans(n_clusters=100)\n","    km.fit(embed)\n","\n","    # getting these centroids\n","    centroids = km.cluster_centers_\n","    medoids = (\n","        pd.DataFrame(embed)\n","        .apply(lambda x: km.score(x.values.reshape(1,-1)), axis=1)\n","        .groupby(km.predict(embed))\n","        .idxmax()\n","    )\n","\n","    # images to plot\n","    img_to_plot = meta_df.index.values[medoids]\n","    \n","    # plotting a light scatter plot\n","    #fig, ax = plt.subplots(figsize=(12,6), dpi=120)\n","    ax.scatter(embed[:,0], embed[:,1], s=2, alpha=0.1, color='black')\n","\n","    # loop adding pictures to plot\n","    for i, img in enumerate(img_to_plot):\n","\n","        img = plt.imread(img)\n","        imagebox = OffsetImage(img, zoom=0.1)\n","        imagebox.image.axes = ax\n","\n","        ab = AnnotationBbox(imagebox, embed[medoids[i]], pad=0)\n","        ax.add_artist(ab)\n","        \n","    # title and other info\n","    ax.set_title(title)\n","    ax.set_xlabel('first UMAP dimension')\n","    ax.set_ylabel('second UMAP dimension')\n","    plt.grid(b=None)\n","    ax.set_xticks([]); ax.set_yticks([])\n","        \n","def plot_embedding(embed, zeca_embed, title, colors):\n","    \n","    # opening figure\n","    #fig, ax = plt.subplots(figsize=(12,6), dpi=120)\n","    \n","    # running scatterplot for all dogs and zeca\n","    plt.scatter(embed[:,0], embed[:,1], s=2, c=colors, cmap='gist_rainbow')\n","    plt.scatter(zeca_embed[:,0], zeca_embed[:,1], s=300, c='black', label='Zeca', marker='*')\n","    \n","    # title and other info\n","    plt.title(title)\n","    plt.xlabel('first UMAP dimension')\n","    plt.ylabel('second UMAP dimension')\n","    plt.xticks([]); plt.yticks([])\n","    plt.grid(b=None)\n","    plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YE7gBUvSEoZs"},"source":["## Data\n","\n","The data is divided into 120 folders, each representing a breed, that contain several dog pictures each. The `build_metadata` function builds a simple dataframe which contains a single column `breed` and the path to the corresponding image as index."]},{"cell_type":"code","metadata":{"id":"JWBMCavTEoZs","colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"status":"ok","timestamp":1636118684830,"user_tz":300,"elapsed":44825,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"4b3ad5de-a522-4a45-9b6d-dcd249bc6190"},"source":["# reading data\n","meta_df = build_metadata()\n","meta_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["121it [00:42,  2.86it/s]\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>breed</th>\n","    </tr>\n","    <tr>\n","      <th>pet_id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>/content/drive/MyDrive/CS465/project/Images/n02113186-Cardigan/n02113186_1030.jpg</th>\n","      <td>Cardigan</td>\n","    </tr>\n","    <tr>\n","      <th>/content/drive/MyDrive/CS465/project/Images/n02113186-Cardigan/n02113186_10816.jpg</th>\n","      <td>Cardigan</td>\n","    </tr>\n","    <tr>\n","      <th>/content/drive/MyDrive/CS465/project/Images/n02113186-Cardigan/n02113186_10077.jpg</th>\n","      <td>Cardigan</td>\n","    </tr>\n","    <tr>\n","      <th>/content/drive/MyDrive/CS465/project/Images/n02113186-Cardigan/n02113186_10505.jpg</th>\n","      <td>Cardigan</td>\n","    </tr>\n","    <tr>\n","      <th>/content/drive/MyDrive/CS465/project/Images/n02113186-Cardigan/n02113186_10535.jpg</th>\n","      <td>Cardigan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                       breed\n","pet_id                                                      \n","/content/drive/MyDrive/CS465/project/Images/n02...  Cardigan\n","/content/drive/MyDrive/CS465/project/Images/n02...  Cardigan\n","/content/drive/MyDrive/CS465/project/Images/n02...  Cardigan\n","/content/drive/MyDrive/CS465/project/Images/n02...  Cardigan\n","/content/drive/MyDrive/CS465/project/Images/n02...  Cardigan"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"HNoDsjiREoZu"},"source":["As expected, we have 120 breeds. Also, we have 20580 images, as a I joined the train and test sets of the original dataset, as I need the most data I can get."]},{"cell_type":"code","metadata":{"id":"9He0Jz5_EoZu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636118684831,"user_tz":300,"elapsed":5,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"aa2f7dbf-b6d0-470f-9719-269fa0e34d20"},"source":["# number of unique breeds after filter\n","print('number of unique breeds:', meta_df['breed'].nunique())\n","print('number of rows in the dataframe:', meta_df['breed'].shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of unique breeds: 120\n","number of rows in the dataframe: 20580\n"]}]},{"cell_type":"markdown","metadata":{"id":"bGIRl6_6EoZz"},"source":["We reserve the images' paths for use later:"]},{"cell_type":"code","metadata":{"id":"XE00Z4sQEoZ0"},"source":["# creating list with paths\n","paths = meta_df.index.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmJc9dDrEoZ1"},"source":["## Feature extraction \n","\n","The first step is extracting features from the images using a pretrained neural network. I chose `Xception` based on its good results on this [Kaggle Kernel](https://www.kaggle.com/gaborfodor/dog-breed-pretrained-keras-models-lb-0-3/#data), and for it being relatively lightweight for quick inference."]},{"cell_type":"code","metadata":{"id":"KIp2B5X-EoZ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636118690130,"user_tz":300,"elapsed":5302,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"26d9cf7d-6749-4e9b-fff3-5c95bb49f500"},"source":["# using a pre-trained net\n","# Xception Model\n","from tensorflow.keras.applications.xception import Xception\n","# VGG19 Model\n","from tensorflow.keras.applications.vgg19 import VGG19\n","# DenseNet201 Model\n","from tensorflow.keras.applications.densenet import DenseNet201\n","# InceptionV3 Model\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","\n","from tensorflow.keras.preprocessing import image\n","\n","# instance of feature extractor\n","# extractor = VGG19(include_top=False, pooling='avg')\n","# extractor = VGG19(include_top=False, pooling='max')\n","# extractor = Xception(include_top=False, pooling='avg')\n","# extractor = Xception(include_top=False, pooling='max')\n","extractor = DenseNet201(include_top=False, pooling='none')\n","# extractor = DenseNet201(include_top=False, pooling='avg')\n","# extractor = DenseNet201(include_top=False, pooling='max')\n","# extractor = InceptionV3(include_top=False, pooling='avg')\n","# extractor = InceptionV3(include_top=False, pooling='max')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n","74842112/74836368 [==============================] - 1s 0us/step\n","74850304/74836368 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"3S9vHo8p5o-V"},"source":["Before doing the feature extrating we need to do make use of transfer learning and fine tuning to improve the prediciton qulaity of our model"]},{"cell_type":"code","metadata":{"id":"ZAQqepko56Pn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636122392870,"user_tz":300,"elapsed":1522,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"143a5670-6972-4365-a268-d890dcfdd10d"},"source":["# Source: https://www.tensorflow.org/guide/keras/transfer_learning\n","# freeze the base extractor model\n","extractor.trainable = False\n","\n","# create new model to put on top of the base extractor model\n","inputs = keras.Input(shape=(244, 244, 3))\n","x = extractor(inputs, training=False)\n","\n","# Source: Assignment 3\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, Conv1D, MaxPooling2D\n","\n","# x = Conv2D(32, kernel_size=(3, 3), activation='softmax')(x)\n","# x = Conv2D(64, kernel_size=(3, 3), activation='softmax')(x)\n","# # (1) First batch normalization layer\n","# x = keras.layers.BatchNormalization()(x)\n","# # (2) Two Convolution Layers\n","# # doubleing the number of output filters each convolution\n","# x = Conv2D(128, (3, 3), activation='softmax')(x)\n","# x = Conv2D(256, (3, 3), activation='softmax')(x)\n","# # (1) Second batch normalization layer\n","# x = keras.layers.BatchNormalization()(x)\n","\n","x = MaxPooling2D(pool_size=(2, 2))(x)\n","x = Dropout(0.25)(x)\n","x = Flatten()(x)\n","x = Dense(meta_df['breed'].nunique(), activation='softmax')(x)\n","x = Dropout(0.5)(x)\n","\n","# A Dense classifier with 120 units (120 classification)\n","outputs = keras.layers.Dense(meta_df['breed'].nunique())(x)\n","extractor = keras.Model(inputs, outputs)\n","extractor.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 244, 244, 3)]     0         \n","_________________________________________________________________\n","model (Functional)           (None, 7, 7, 120)         18552504  \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 3, 3, 120)         0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 3, 3, 120)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1080)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 120)               129720    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 120)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 120)               14520     \n","=================================================================\n","Total params: 18,696,744\n","Trainable params: 144,240\n","Non-trainable params: 18,552,504\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"MxoW1YGqcr8p"},"source":["#Gather the Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5tDptOzxSfi","executionInfo":{"status":"ok","timestamp":1636121359494,"user_tz":300,"elapsed":983,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"7d3be77e-3e61-44cd-857b-26b751ea51a1"},"source":["import tensorflow.data.experimental\n","from tensorflow.keras.utils import image_dataset_from_directory\n","train_ds = image_dataset_from_directory(\n","    \"/content/drive/MyDrive/CS465/project/Images\", labels='inferred', label_mode='categorical',\n","    class_names=None, color_mode='rgb', batch_size=32, image_size=(244, 244),\n","    shuffle=True, seed=100, validation_split=.1, subset='training',\n","    interpolation='bilinear', crop_to_aspect_ratio=True\n",")\n","\n","print(\"Number of training samples: %d\" % tensorflow.data.experimental.cardinality(train_ds))\n","#print(\n","#    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n","#)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20580 files belonging to 120 classes.\n","Using 18522 files for training.\n","Number of training samples: 579\n"]}]},{"cell_type":"code","metadata":{"id":"zAEonWjWck70","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636119418853,"user_tz":300,"elapsed":492430,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"017c2991-7a96-4564-e517-9688dea34097"},"source":["########################################\n","# get entire image dataset\n","length = len(paths)\n","mid_index = length//10\n","first_nth = paths[:mid_index]\n","dataset = read_images(first_nth)\n","images, ids = dataset[0], dataset[1]\n","\n","# get entire image dataset\n","#dataset, ids = read_images(paths)\n","########################################"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2058/2058 [08:05<00:00,  4.24it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"S2oE-Z_kdoWX"},"source":["#Train the custom top model"]},{"cell_type":"code","metadata":{"id":"Czd0J8b3cn_P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636131550695,"user_tz":300,"elapsed":9154322,"user":{"displayName":"Sean Hansen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBcJrFhqvFYpHZuJXKEa061-vX8h5CooYKiyDOwQ=s64","userId":"18075402617456141681"}},"outputId":"0cfecc47-e2ca-4772-bb72-a0c576517b27"},"source":["# Train the new model\n","from tensorflow.keras.optimizers import Adam\n","extractor.compile(optimizer=Adam(), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])\n","\n","extractor.fit(train_ds, batch_size=128, epochs=10, verbose=1,shuffle=True, use_multiprocessing=True)# validation_split=0.2, "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","579/579 [==============================] - 1114s 2s/step - loss: 4.7899 - categorical_accuracy: 0.0092\n","Epoch 2/10\n","579/579 [==============================] - 797s 1s/step - loss: 4.7844 - categorical_accuracy: 0.0119\n","Epoch 3/10\n","579/579 [==============================] - 799s 1s/step - loss: 4.7829 - categorical_accuracy: 0.0112\n","Epoch 4/10\n","579/579 [==============================] - 972s 2s/step - loss: 4.7821 - categorical_accuracy: 0.0110\n","Epoch 5/10\n","579/579 [==============================] - 982s 2s/step - loss: 4.7817 - categorical_accuracy: 0.0111\n","Epoch 6/10\n","579/579 [==============================] - 930s 2s/step - loss: 4.7820 - categorical_accuracy: 0.0113\n","Epoch 7/10\n","579/579 [==============================] - 969s 2s/step - loss: 4.7807 - categorical_accuracy: 0.0105\n","Epoch 8/10\n","579/579 [==============================] - 825s 1s/step - loss: 4.7799 - categorical_accuracy: 0.0125\n","Epoch 9/10\n","579/579 [==============================] - 879s 2s/step - loss: 4.7772 - categorical_accuracy: 0.0112\n","Epoch 10/10\n","579/579 [==============================] - 885s 2s/step - loss: 4.7687 - categorical_accuracy: 0.0130\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f388c827990>"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"AdCfHQ5sEoZ2"},"source":["The function `extract_features` gets a list of paths, an `extractor` (the Xception net in this case), and returns a dataframe with features. We save the dataframe so we don't need to run the process all the time (it takes ~15 minutes on my machine)."]},{"cell_type":"code","metadata":{"id":"2ehzbytAEoZ2"},"source":["# if we havent extracted features, do it\n","if not os.path.exists('/content/drive/MyDrive/CS465/project/transferLearning_densenet201_avgPooling_features.csv'):\n","    features_df = extract_features(paths, extractor)\n","    features_df.to_csv('/content/drive/MyDrive/CS465/project/transferLearning_densenet201_avgPooling_features.csv')\n","    \n","# read features\n","features_df = pd.read_csv('/content/drive/MyDrive/CS465/project/transferLearning_densenet201_avgPooling_features.csv', index_col='pet_id')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"se5SgborEoZ3"},"source":["As the extraction pipeline can't process some of the images, we need to realign our metadata index with the extraction's index, so they have the same images, in the same order:"]},{"cell_type":"code","metadata":{"id":"m0WaMpUEEoZ4"},"source":["# realign index with main df\n","meta_df = meta_df.loc[features_df.index]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzZYRuM8EoZ4"},"source":["## Modeling\n","\n","Now we can start modeling. We'll build a Logistic Regression to classify breeds on top of the Xception's features, and apply this model on a picture of Zeca. However, for the sake of explainability, we'll also create a nearest-neighbors model, so we can supply prototypes, comparable dogs to Zeca that can support the model's predictions.\n","\n","Let's start with data preparation!"]},{"cell_type":"markdown","metadata":{"id":"_ILD-H7_EoZ5"},"source":["### Data preparation\n","\n","Just explicitly splitting our design matrix `X` and target variable `y` into train and test sets (90%/10% split, stratified). We encode `y` using `LabelEncoder` as this is a multiclass classification problem."]},{"cell_type":"code","metadata":{"id":"URQF7GfcEoZ6"},"source":["# label encoder for target and splitter\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","# defining design matrix\n","X = features_df.copy().values\n","\n","# defining target\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(meta_df['breed'])\n","\n","# splitting train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8kR9yItEoZ6"},"source":["### Dimensionality reduction with PCA\n","\n","We run PCA in the Xception's features. We have two reasons for that:\n","\n","1. **Efficiency.** PCA can retain 96% of variance with half the features (1024 instead of 2048). This helps everything run faster further in the pipeline.\n","2. **Whitening.** Whitening is the PCA's capability of returning a matrix where features have mean 0, variance 1, and are uncorrelated. This will be important as it allows us to interpret the  Logistic Regression coefficients as feature importances.\n","\n","We fit PCA with the following code:"]},{"cell_type":"code","metadata":{"id":"y5KNZG3sEoZ6"},"source":["# PCA\n","from sklearn.decomposition import PCA\n","\n","# instance of PCA\n","# pca = PCA(n_components=1024, whiten=True)\n","pca = PCA(n_components=512, whiten=True)\n","\n","# applying PCA to data\n","# must only fit on train data\n","history = pca.fit(X_train)\n","\n","# checking explained variance\n","explained_var = pca.explained_variance_ratio_.sum()\n","print(f'PCA explained variance: {explained_var:.4f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsEA6_PPEoZ7"},"source":["### Logistic Regression\n","\n","We then proceed to fit and evaluate a Logistic Regression. It's fairly easy and fast to fit it:"]},{"cell_type":"code","metadata":{"id":"mLkWndSIEoZ7"},"source":["# logistic regression and eval metrics\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, log_loss\n","\n","# instance of logistic regression\n","# C = Inverse of regularization strength\n","lr = LogisticRegression(C=1e-2, multi_class='multinomial', penalty='l2', max_iter=200)\n","\n","# fitting to train\n","lr.fit(pca.transform(X_train), y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_75OZ2eMEoZ8"},"source":["# evaluating\n","val_preds = lr.predict_proba(pca.transform(X_test))\n","\n","# test metrics\n","print(f'Accuracy: {accuracy_score(y_test, np.argmax(val_preds, axis=1)):.3f}')\n","print(f'Log-loss: {log_loss(y_test, val_preds):.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUl3FVNa_yLM"},"source":["#Original Results\n","*   Accuracy: 0.822\n","*   Log-loss: 0.774\n","\n","#Xception Average Pooling\n","*   Accuracy: 0.823\n","*   Log-loss: 0.736\n","\n","#Xception Max Pooling\n","*   Accuracy: 0.816\n","*   Log-loss: 0.797\n","\n","#VGG19 Average Pooling\n","*   Accuracy: 0.706\n","*   Log-loss: 1.373\n","\n","#VGG19 Max Pooling\n","*   Accuracy: 0.661\n","*   Log-loss: 1.486\n","\n","#DenseNet201 Average Pooling\n","*   Accuracy: 0.870\n","*   Log-loss: 0.636\n","\n","#DenseNet201 Max Pooling\n","*   Accuracy: 0.856\n","*   Log-loss: 0.664\n","\n","#InceptionV3 Average Pooling\n","*   Accuracy: 0.819\n","*   Log-loss: 0.730\n","\n","#InceptionV3 Max Pooling\n","*   Accuracy: 0.802\n","*   Log-loss: 0.769"]},{"cell_type":"markdown","metadata":{"id":"wX6OUjuPEoZ9"},"source":["### Predicting breed"]},{"cell_type":"code","metadata":{"id":"o8I4sVOKIUHJ"},"source":["# define test image path\n","testImgPath = '/content/drive/MyDrive/CS465/project/greatWhitePyranees.jpeg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxoIqkUUEoZ9"},"source":["# features from zeca\n","features_zeca = extract_features([f'/content/drive/MyDrive/CS465/project/greatWhitePyranees.jpeg'], extractor)\n","\n","# predictions for zeca\n","preds_zeca = lr.predict_proba(pca.transform(features_zeca))[0]\n","preds_zeca = pd.Series(preds_zeca, index=label_encoder.classes_)\n","preds_zeca.sort_values(ascending=False).to_frame().head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mk8qSGUUEoZ-"},"source":["### Explanations via embeddings and prototypes\n","\n","One easy and effective method that I usually apply for explaining models is trying to transform them in a kNN (yeah, nearest neighbors!), as it outputs *hard examples* to support the model's decisions (or *prototypes*, as in the literature). How do we transform our Xception + PCA + Logistic Regession pipeline in a kNN, though? I'll show you two ways:\n","\n","1. **Direct, naive way:** Just search for Zeca's neighbors in the `Xception` feature space after `PCA`\n","2. **Scale by Logistic Regression coefficients:** we apply a bit of **supervision** on the `Xception` + `PCA` embedding, scaling its features proportionally to the weights of the Logistic Regression.\n","\n","Let us check how they perform. We start by importing `NNDescent`, a fast, efficient method to perform approximate nearest neighbor search:"]},{"cell_type":"code","metadata":{"id":"EwcOt6iaEoZ_"},"source":["# nearest neighbors\n","from pynndescent import NNDescent"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Gu-d_-JEoZ_"},"source":["### Direct, naive embedding\n","\n","Now, we search for Zeca's comparables in a naive way. It consists of creating an index on the `Xception` + `PCA` embedding, and then searching for zeca's neighbors in this index. The function `get_prototypes_report` takes care of that for us, and shows pictures and most frequent breeds for Zeca's neighbors:"]},{"cell_type":"code","metadata":{"id":"RoQuCi4vEoZ_"},"source":["# creating NN index \n","index_direct = NNDescent(pca.transform(X))\n","\n","# running\n","get_prototypes_report(f'/content/drive/MyDrive/CS465/project/greatWhitePyranees.jpeg', index_direct, extractor, pca.transform, meta_df, features_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6sWBxynEoaA"},"source":["We start off OK on the first 10 dogs, but we still get neighbors that don't make much sense, like the `EntleBucher` or `Bouvier_des_Flandres`. Let us then improve that by applying a bit of **supervision** using the logistic regression's weights."]},{"cell_type":"markdown","metadata":{"id":"rFT6agyiEoaA"},"source":["### Scale by Logistic Regression coefficients \n","\n","Let us perform a very simple modification to the embedding that our nearest neighbor method builds its index on. We use the fact that we can interpret the absolute value of the coefficients of the Logistic Regression as feature importances (as allowed by the whitening process), and scale the embedding features proportionally to these coefficients.\n","\n","For instance, we can check that there are some features with nearly 10x more importance than others: "]},{"cell_type":"code","metadata":{"id":"1gRgYX5sEoaB"},"source":["# checking feature importance\n","np.abs(lr.coef_).sum(axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMiuw1slEoaB"},"source":["So, when we scale the embedding this way, the Logistic Regression's most important features will have greater variance, and thus will have more weight when we search for Zeca's nearest neighbors:"]},{"cell_type":"code","metadata":{"id":"EPLd3-1REoaC"},"source":["# function to 'supervise' embedding given coefficients of logreg\n","lr_coef_transform = lambda x: np.abs(lr.coef_).sum(axis=0) * pca.transform(x)\n","\n","# creating NN index \n","index_logistic = NNDescent(lr_coef_transform(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cL0EIXxUEoaC"},"source":["The results are much better:"]},{"cell_type":"code","metadata":{"id":"YCmRgfvVEoaC"},"source":["# running\n","get_prototypes_report(f'/content/drive/MyDrive/CS465/project/greatWhitePyranees.jpeg', index_logistic, extractor, lr_coef_transform, meta_df, features_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kwcRbiREoaD"},"source":["The prototypes agree a lot with the Logistic Regression results, and they're gonna be a solid argument for my family that the model works."]},{"cell_type":"markdown","metadata":{"id":"ZxBXCFOyEoaD"},"source":["### Digging deeper: Why did the supervision work?\n","\n","Why did simple scaling improve prototype quality by so much? My hypothesis is based on the curse on dimensionality. To check that, let us compare the 2D embedding generated by `UMAP` from the naive and scaled approaches.\n","\n","We generate the embeddings with the following code:"]},{"cell_type":"code","metadata":{"id":"Gog9In1gEoaE"},"source":["# UMAP for dimension reduction\n","from umap.umap_ import UMAP\n","\n","# building embedding\n","umap_direct = UMAP()\n","embed_direct = umap_direct.fit_transform(pca.transform(X))\n","\n","# predicting zeca\n","zeca_embed_direct = umap_direct.transform(pca.transform(features_zeca))\n","\n","# building embedding\n","umap_logistic = UMAP()\n","embed_logistic = umap_logistic.fit_transform(lr_coef_transform(X))\n","\n","# predicting zeca\n","zeca_embed_logistic = umap_logistic.transform(lr_coef_transform(features_zeca))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pyd9VSTMEoaE"},"source":["And plot them below:"]},{"cell_type":"code","metadata":{"id":"kMFFn8cLEoaE"},"source":["# opening figure\n","plt.figure(figsize=(16, 6), dpi=150)\n","\n","# plotting 2D reduction of naive embedding\n","plt.subplot(1, 2, 1)\n","plot_embedding(embed_direct, zeca_embed_direct, 'Color is Breed: embedding directly from network', y)\n","\n","# plotting 2D reduction of scaled embedding\n","plt.subplot(1, 2, 2)\n","plot_embedding(embed_logistic, zeca_embed_logistic, 'Color is Breed: embedding scaled by logistic regression weights', y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELxlekOgEoaF"},"source":["Breed is color-coded in the plots. The naive embedding plot, in the left-hand side, shows reasonable structure, with some clear clusters of breeds clumping together. However, there's a \"hubness\" problem: there's a central clump of dog images where there's a lot of mix between breeds. I make my case that this is the curse of dimensionality at play: the 1024 features we get from the `Xception` and `PCA` are suited to a much more general problem of object idenfitication and are \"too sparse\" for our dog breed classification problem. Thus, we end up comparing pictures on features that don't make sense for our specific problem, making dogs that are different appear the same (and the contrary as well).\n","\n","In the right-hand plot, built from the scaled embedding, we get much tighter, cleaner clusters, with no \"hubness\" at all. The scaling process acts like a \"filter\" letting we only compare pictures of dogs on the features that are important for our specific task of dog breed identification, as determined by our model. It's like **learning a distance** between our entities given our task."]},{"cell_type":"markdown","metadata":{"id":"r4R0J2dlEoaF"},"source":["For your amusement, I can also generate these plots using dog pictures. Here's what I call the **Dog Atlas**:"]},{"cell_type":"code","metadata":{"id":"fAasEeBvEoaF"},"source":["# opening figure\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6), dpi=150)\n","\n","# plotting atlas\n","plot_dog_atlas(embed_direct, meta_df, 'Dog Atlas: naive embedding', ax[0])\n","plot_dog_atlas(embed_logistic, meta_df, 'Dog Atlas: scaled embedding', ax[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4lNePbEnEoaG"},"source":["Cool! It's a fluffier way to see that the scaled embedding is better :)"]},{"cell_type":"markdown","metadata":{"id":"ByH1Uvi7EoaG"},"source":["### Final Remarks\n","\n","Cool! We solved the mistery of Zeca's breed. We did not solve age, though, as we did not have the labels. I'll try on a next project.\n","\n","Thank you very much for reading! Comments and feedbacks are appreciated :)"]},{"cell_type":"code","metadata":{"id":"y88XxSM48CcH"},"source":["import matplotlib.pyplot as plt\n","# summarize history for accuracy\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]}]}